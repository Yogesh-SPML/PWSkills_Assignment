{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an activation function in the context of artificial neural networks?"
      ],
      "metadata": {
        "id": "mGtxeOjdjIdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An activation function is a mathematical function that is used in artificial neural networks to determine the output of a neuron. It takes the weighted sum of the inputs to the neuron and applies a non-linear function to it, which allows the neuron to learn non-linear relationships between the inputs and outputs. Some common activation functions include the sigmoid function, the ReLU function, and the tanh function."
      ],
      "metadata": {
        "id": "aKTe50TFjTeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are some common types of activation functions used in neural networks?"
      ],
      "metadata": {
        "id": "3VIU20RWjc20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ReLU\n",
        "2. Tanh\n",
        "3. Sigmoid\n",
        "4. Leaky ReLU"
      ],
      "metadata": {
        "id": "AXmlffXPjgV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do activation functions affect the training process and performance of a neural network?"
      ],
      "metadata": {
        "id": "djY7XnVwjqN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions play a critical role in the training process and performance of a neural network. They help to introduce non-linearity into the network, which is necessary for learning complex relationships between the inputs and outputs. The choice of activation function can have a significant impact on the performance of the network, as different activation functions have different properties that can affect the network's ability to learn. For example, the sigmoid function can suffer from the \"vanishing gradient\" problem, where the gradients become very small as the input approaches 0 or 1, which can slow down the training process. The ReLU function, on the other hand, is faster to compute and can help to mitigate the vanishing gradient problem, but can suffer from the \"dying ReLU\" problem, where neurons can become stuck at 0 and stop learning. Overall, the choice of activation function depends on the specific problem being solved and the characteristics of the data."
      ],
      "metadata": {
        "id": "tTJGeSz8jw2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
      ],
      "metadata": {
        "id": "59tynT14j_5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sigmoid activation function is a mathematical function that maps any input to a value between 0 and 1. It is a smooth, S-shaped curve that is commonly used in artificial neural networks. One advantage of the sigmoid function is that it is easy to compute and has a well-defined derivative, which makes it easy to use in gradient-based optimization algorithms. However, the sigmoid function can suffer from the \"vanishing gradient\" problem, where the gradients become very small as the input approaches 0 or 1, which can slow down the training process. Additionally, the output of the sigmoid function is always positive, which can make it difficult to model negative inputs."
      ],
      "metadata": {
        "id": "679fpZK2kES4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
      ],
      "metadata": {
        "id": "3k9DP-zAkQfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU activation function is a mathematical function that returns the input if it is positive, and 0 otherwise. It is a piecewise linear function that is commonly used in artificial neural networks. One advantage of the ReLU function is that it is faster to compute than the sigmoid function, which can speed up the training process. Additionally, the ReLU function does not suffer from the \"vanishing gradient\" problem that can affect the sigmoid function, which can make it easier to train deep neural networks. Unlike the sigmoid function, the output of the ReLU function can be negative, which makes it easier to model negative inputs."
      ],
      "metadata": {
        "id": "W3F2DFslkTIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
      ],
      "metadata": {
        "id": "OrI7iYSXks7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU activation function has several benefits over the sigmoid function. First, it is faster to compute, which can speed up the training process. Second, it does not suffer from the \"vanishing gradient\" problem that can affect the sigmoid function, which can make it easier to train deep neural networks. Finally, the output of the ReLU function can be negative, which makes it easier to model negative inputs."
      ],
      "metadata": {
        "id": "Re2_XVCukwvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n"
      ],
      "metadata": {
        "id": "fXMItAP1k1uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leaky ReLU is a variant of the ReLU activation function that addresses the vanishing gradient problem. It works by allowing small, non-zero values for negative inputs, which helps to prevent neurons from becoming \"dead\" and unresponsive to inputs. By introducing this small amount of \"leakage\" into the function, the gradients can still flow backwards through the network during training, which can help to speed up the training process and improve performance."
      ],
      "metadata": {
        "id": "kZavn3pfk9Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the purpose of the softmax activation function? When is it commonly used?"
      ],
      "metadata": {
        "id": "PR2grSZgk_7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax activation function is a mathematical function that maps a vector of real numbers to a probability distribution. It is commonly used in multi-class classification problems, where the goal is to predict the probability of each class given the input. The softmax function ensures that the probabilities of all classes add up to 1, which makes it easier to interpret the output of the model and make predictions."
      ],
      "metadata": {
        "id": "BJbg6AOZlZS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
      ],
      "metadata": {
        "id": "nz_toLsplaGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperbolic tangent (tanh) activation function is a mathematical function that maps any input to a value between -1 and 1. It is similar to the sigmoid function, but it is shifted and scaled to have a range between -1 and 1, which can make it easier to model negative inputs. Like the sigmoid function, the tanh function is smooth and has a well-defined derivative, which can make it easy to use in gradient-based optimization algorithms. However, like the sigmoid function, the tanh function can also suffer from the \"vanishing gradient\" problem, which can slow down the training process."
      ],
      "metadata": {
        "id": "oNGbaFm6llQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xj2VS5wKcwWw"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}